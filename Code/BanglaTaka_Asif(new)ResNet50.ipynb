{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa94bRQ2y4C397QAD8SkLS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asifmanowar9/BanglaTaka/blob/main/Code/BanglaTaka_Asif(new)ResNet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iez9XErCGXMF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee9d6aa3"
      },
      "source": [
        "# Task\n",
        "Classify Bangladeshi banknote images using a pre-trained ResNet50 model, utilizing the dataset located in the Google Drive folder 'https://drive.google.com/drive/folders/13P5Soos4thSeu9Su62lHVENzrSF3kxCA'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef972492"
      },
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "### Subtask:\n",
        "Mount your Google Drive to access the dataset directly from the provided link: 'https://drive.google.com/drive/folders/13P5Soos4thSeu9Su62lHVENzrSF3kxCA'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0153e4a5"
      },
      "source": [
        "**Reasoning**:\n",
        "To mount Google Drive, I need to import the drive module from google.colab and then call the drive.mount() function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a8058e1",
        "outputId": "15982b2b-21a6-4499-b9fb-c8f5ddc260aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d31f4e11"
      },
      "source": [
        "## Load and Preprocess Dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the images from the mounted Google Drive, preprocess them (e.g., resize to a standard input size for ResNet50 like 224x224, normalize pixel values), and create corresponding labels based on the folder structure. Make sure to handle image loading and basic augmentation for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "drive_path = '/content/drive/MyDrive/aa/dataset' # Replace 'Your_Shortcut_Name' with the actual name of your shortcut\n",
        "\n",
        "if os.path.exists(drive_path):\n",
        "    print(f\"Listing contents of '{drive_path}':\")\n",
        "    for item in os.listdir(drive_path):\n",
        "        print(item)\n",
        "else:\n",
        "    print(f\"Error: The path '{drive_path}' does not exist. Please check your shortcut name and path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zeAde-cI72B",
        "outputId": "89852725-044d-4353-d043-0a9e6ce8f617"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of '/content/drive/MyDrive/aa/dataset':\n",
            "100\n",
            "200\n",
            "500\n",
            "1000\n",
            "50\n",
            "2\n",
            "5\n",
            "10\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare Data\n",
        "\n",
        "defines image dimensions and batch size, sets the corrected dataset root path, loads the dataset, prints class names, defines and applies a normalization function, and verifies the preprocessing."
      ],
      "metadata": {
        "id": "OoELxmxdJq_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define image dimensions and batch size\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32\n",
        "\n",
        "# 2. Set the dataset_root variable to the confirmed path\n",
        "dataset_root = '/content/drive/MyDrive/aa/dataset'\n",
        "\n",
        "print(f\"Attempting to load dataset from: {dataset_root}\")\n",
        "\n",
        "try:\n",
        "    # 3. Load the image dataset using image_dataset_from_directory\n",
        "    raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_root,\n",
        "        labels='inferred',\n",
        "        label_mode='int',\n",
        "        image_size=(img_height, img_width),\n",
        "        interpolation='nearest',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    # 4. Print class names and approximate number of images\n",
        "    class_names = raw_train_ds.class_names\n",
        "    print(f\"\\nClasses found: {class_names}\")\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "    # Corrected: Handle cardinality carefully\n",
        "    total_batches_cardinality = tf.data.experimental.cardinality(raw_train_ds)\n",
        "    if total_batches_cardinality == tf.data.AUTOTUNE:\n",
        "        print(\"Total number of batches could not be determined (tf.data.AUTOTUNE). Dataset size might be very large.\")\n",
        "    else:\n",
        "        # Ensure we call .numpy() only if it's a TensorFlow Tensor\n",
        "        total_batches = total_batches_cardinality.numpy() if hasattr(total_batches_cardinality, 'numpy') else total_batches_cardinality\n",
        "        total_images = total_batches * batch_size\n",
        "        print(f\"Approximate total number of images in the dataset: {total_images}\")\n",
        "\n",
        "    # 5. Preprocessing function for normalization (corrected)\n",
        "    def preprocess(image, label):\n",
        "        image = tf.cast(image, tf.float32) / 255.0 # Cast image to float32 first, then divide\n",
        "        return image, label\n",
        "\n",
        "    # 6. Apply preprocessing to the dataset\n",
        "    processed_train_ds = raw_train_ds.map(preprocess)\n",
        "    print(\"\\nImage normalization preprocessing applied to the dataset.\")\n",
        "\n",
        "    # 7. Verify preprocessing by taking one batch\n",
        "    for image_batch, labels_batch in processed_train_ds.take(1):\n",
        "        print(f\"\\nShape of one image batch after preprocessing: {image_batch.shape}\")\n",
        "        print(f\"Min pixel value in one image batch: {tf.reduce_min(image_batch)}\")\n",
        "        print(f\"Max pixel value in one image batch: {tf.reduce_max(image_batch)}\")\n",
        "        print(f\"Shape of one label batch: {labels_batch.shape}\")\n",
        "        break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure the 'dataset_root' is correct and points to a directory containing image subfolders (classes).\")\n",
        "    print(\"Example: '/content/drive/MyDrive/DSA_Project_Folder'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSK-3aVnJsCc",
        "outputId": "af55ab04-19b0-412a-b038-abe0b98658e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load dataset from: /content/drive/MyDrive/aa/dataset\n",
            "Found 10634 files belonging to 9 classes.\n",
            "\n",
            "Classes found: ['10', '100', '1000', '2', '20', '200', '5', '50', '500']\n",
            "Number of classes: 9\n",
            "Approximate total number of images in the dataset: 10656\n",
            "\n",
            "Image normalization preprocessing applied to the dataset.\n",
            "\n",
            "Shape of one image batch after preprocessing: (32, 128, 128, 3)\n",
            "Min pixel value in one image batch: 0.0\n",
            "Max pixel value in one image batch: 1.0\n",
            "Shape of one label batch: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Dataset\n",
        "\n",
        "Split the prepared dataset into training, validation, and test sets to properly evaluate the model's generalization capabilities.\n",
        "\n",
        "This ensures that the model is trained on one part of the data, fine-tuned on another (validation), and finally evaluated on unseen data (test set) to provide an unbiased assessment of its performance.\n",
        "\n",
        "**Instructions:**\n",
        "1.  **Determine Total Images**: We'll use the `total_images` variable calculated in the previous step as an approximation for the dataset size.\n",
        "2.  **Define Split Ratios**: We will use a standard split of 70% for training, 15% for validation, and 15% for testing.\n",
        "3.  **Calculate Batches**: The dataset is already batched, so we will determine the number of batches for each split based on the ratios and the overall number of batches.\n",
        "4.  **Create Subsets**: TensorFlow's `tf.data.Dataset` API provides efficient methods (`take`, `skip`) for creating these subsets.\n",
        "5.  **Optimize Performance**: Apply `.cache()` and `.prefetch()` to each dataset split to improve training efficiency. `.cache()` keeps images in memory after the first epoch, and `.prefetch()` overlaps data preprocessing and model execution."
      ],
      "metadata": {
        "id": "IRLp9NpYKOUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Assuming `processed_train_ds` and `batch_size` are available from previous steps\n",
        "# `total_images` was also calculated, but it's more robust to calculate based on `processed_train_ds` cardinality if possible.\n",
        "# Let's re-evaluate total batches to be sure, assuming `raw_train_ds` is the source.\n",
        "\n",
        "# Get the total number of batches from the processed dataset\n",
        "total_batches_cardinality = tf.data.experimental.cardinality(processed_train_ds)\n",
        "if total_batches_cardinality == tf.data.AUTOTUNE:\n",
        "    # Fallback to the approximate total_images if cardinality is not concrete\n",
        "    if 'total_images' in locals():\n",
        "        total_images_approx = total_images # Use previously calculated approximate total_images\n",
        "        total_batches_approx = math.ceil(total_images_approx / batch_size)\n",
        "    else:\n",
        "        print(\"Warning: Cannot determine total number of batches or images for splitting. Please ensure 'processed_train_ds' is properly formed.\")\n",
        "        # For demonstration, assign a default value or raise an error\n",
        "        total_batches_approx = 100 # Placeholder for a real scenario, this would cause issues\n",
        "else:\n",
        "    total_batches_approx = total_batches_cardinality.numpy() # Convert Tensor to Python int\n",
        "\n",
        "print(f\"Total batches in processed dataset: {total_batches_approx}\")\n",
        "\n",
        "# Define split ratios\n",
        "train_split_ratio = 0.7\n",
        "val_split_ratio = 0.15\n",
        "test_split_ratio = 0.15\n",
        "\n",
        "# Calculate number of batches for each split\n",
        "train_batches = int(total_batches_approx * train_split_ratio)\n",
        "val_batches = int(total_batches_approx * val_split_ratio)\n",
        "\n",
        "# The remaining batches go to the test set\n",
        "test_batches = total_batches_approx - train_batches - val_batches\n",
        "\n",
        "print(f\"\\nSplitting dataset:\")\n",
        "print(f\"  Training batches: {train_batches}\")\n",
        "print(f\"  Validation batches: {val_batches}\")\n",
        "print(f\"  Test batches: {test_batches}\")\n",
        "\n",
        "# Create the training, validation, and test datasets\n",
        "train_ds = processed_train_ds.take(train_batches)\n",
        "val_ds = processed_train_ds.skip(train_batches).take(val_batches)\n",
        "test_ds = processed_train_ds.skip(train_batches).skip(val_batches)\n",
        "\n",
        "# Verify the number of batches in each split (optional, for debugging)\n",
        "print(f\"\\nBatches after splitting:\")\n",
        "print(f\"  Train dataset batches (actual): {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"  Validation dataset batches (actual): {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "print(f\"  Test dataset batches (actual): {tf.data.experimental.cardinality(test_ds).numpy()}\")\n",
        "\n",
        "# Optimize performance with .cache() and .prefetch()\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"\\nDatasets (train_ds, val_ds, test_ds) created and optimized with .cache() and .prefetch().\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzKN6_tnKOuY",
        "outputId": "80e2a7bc-62c3-45c5-adc2-7cda955d0262"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total batches in processed dataset: 333\n",
            "\n",
            "Splitting dataset:\n",
            "  Training batches: 233\n",
            "  Validation batches: 49\n",
            "  Test batches: 51\n",
            "\n",
            "Batches after splitting:\n",
            "  Train dataset batches (actual): 233\n",
            "  Validation dataset batches (actual): 49\n",
            "  Test dataset batches (actual): 51\n",
            "\n",
            "Datasets (train_ds, val_ds, test_ds) created and optimized with .cache() and .prefetch().\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and Compile CNN Model\n",
        "\n",
        "Now we will construct the Convolutional Neural Network (CNN) model. We'll leverage transfer learning by using a pre-trained base model (ResNet50) and then add custom classification layers on top. Finally, we'll compile the model with suitable settings for training."
      ],
      "metadata": {
        "id": "zSsbrVcCK6t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.applications import ResNet50 # Changed from MobileNetV2\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "# Assuming img_height, img_width, and class_names are defined from previous steps\n",
        "# img_height = 128\n",
        "# img_width = 128\n",
        "# class_names = ['10', '100', '1000', '2', '20', '200', '5', '50', '500'] # Example\n",
        "\n",
        "print(f\"Image dimensions: ({img_height}, {img_width})\")\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "# 1. Load a pre-trained base model (ResNet50) without its top classification layer\n",
        "base_model = ResNet50(input_shape=(img_height, img_width, 3),\n",
        "                           include_top=False,\n",
        "                           weights='imagenet')\n",
        "\n",
        "# 2. Freeze the base model's layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create a new Sequential model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(), # Converts feature maps to a single vector\n",
        "    Dense(128, activation='relu'), # A hidden dense layer\n",
        "    Dense(len(class_names), activation='softmax') # Output layer for multi-class classification\n",
        "])\n",
        "\n",
        "# 4. Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5. Print a summary of the model\n",
        "print(\"\\nModel Architecture Summary:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nCNN model built and compiled successfully using ResNet50.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "ak9AG-VCK7F9",
        "outputId": "f1e4b96b-c35c-404a-8972-a66dfa5b32e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image dimensions: (128, 128)\n",
            "Number of classes: 9\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "\n",
            "Model Architecture Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m1,161\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,851,145\u001b[0m (90.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,851,145</span> (90.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m263,433\u001b[0m (1.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">263,433</span> (1.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CNN model built and compiled successfully using ResNet50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "__5L2lmfLn0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZoZlbwyLoH7",
        "outputId": "92882126-3f70-4113-d9cc-8de7a2e2fdaa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model"
      ],
      "metadata": {
        "id": "z_7tSyfgLpZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10 # You can adjust this number\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"Model training complete. History stored.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av_DpT3gLp95",
        "outputId": "a63710b8-7cfd-4673-992b-4754dacfc9b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2161s\u001b[0m 9s/step - accuracy: 0.2672 - loss: 2.0111 - val_accuracy: 0.5938 - val_loss: 1.5274\n",
            "Epoch 2/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 2s/step - accuracy: 0.5730 - loss: 1.4131 - val_accuracy: 0.7500 - val_loss: 1.0930\n",
            "Epoch 3/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 3s/step - accuracy: 0.7503 - loss: 1.0183 - val_accuracy: 0.7966 - val_loss: 0.8379\n",
            "Epoch 4/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 2s/step - accuracy: 0.8071 - loss: 0.7833 - val_accuracy: 0.8106 - val_loss: 0.6947\n",
            "Epoch 5/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 2s/step - accuracy: 0.8327 - loss: 0.6462 - val_accuracy: 0.8214 - val_loss: 0.6158\n",
            "Epoch 6/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 2s/step - accuracy: 0.8410 - loss: 0.5654 - val_accuracy: 0.8284 - val_loss: 0.5686\n",
            "Epoch 7/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 2s/step - accuracy: 0.8553 - loss: 0.5142 - val_accuracy: 0.8342 - val_loss: 0.5371\n",
            "Epoch 8/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m539s\u001b[0m 2s/step - accuracy: 0.8618 - loss: 0.4785 - val_accuracy: 0.8361 - val_loss: 0.5154\n",
            "Epoch 9/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 3s/step - accuracy: 0.8692 - loss: 0.4519 - val_accuracy: 0.8444 - val_loss: 0.4983\n",
            "Epoch 10/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 2s/step - accuracy: 0.8756 - loss: 0.4308 - val_accuracy: 0.8457 - val_loss: 0.4858\n",
            "Model training complete. History stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save the trained model to drive"
      ],
      "metadata": {
        "id": "y-NDUX9aMSjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/resnet50_trained_model_banglaTaka.keras')"
      ],
      "metadata": {
        "id": "QbdHt1SrRdw4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/resnet50_trained_model_banglaTaka1.keras')"
      ],
      "metadata": {
        "id": "oJ1Pbi14Ri17"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tlvYPlxuJmN9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}